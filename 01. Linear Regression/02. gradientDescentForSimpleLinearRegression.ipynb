{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Gradient Descent Update rule for Linear Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat untill convergence: {\n",
    "\n",
    "\n",
    "- $ \\theta_{0} = \\theta_{0} - \\eta\\frac{1}{m}\\sum_{i=1}^{m}\\left( h_{\\theta}(x^{i})-y^{i} \\right)$\n",
    "\n",
    "\n",
    "- $ \\theta_{1} = \\theta_{1} - \\eta\\frac{1}{m}\\sum_{i=1}^{m}\\left( h_{\\theta}(x^{i})-y^{i} \\right)x^{i}$\n",
    "\n",
    "}\n",
    "\n",
    "where, $ \\theta = [\\theta_{0},\\theta_{1}]$ are y-intersept and slope respectively, $\\eta$ = learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute hypothesis (prediction)\n",
    "def hypothesis(x,theta):\n",
    "    #theta = [theta0,theta1]\n",
    "    return theta[0] + theta[1]*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $ \\theta_{0} = \\theta_{0} - \\eta\\frac{1}{m}\\sum_{i=1}^{m}\\left( h_{\\theta}(x^{i})-y^{i} \\right)$, lets call the term from 1/m to end as gradient0\n",
    "\n",
    "\n",
    "- $ \\theta_{0} = \\theta_{0} - \\eta\\frac{1}{m}\\sum_{i=1}^{m}\\left( h_{\\theta}(x^{i})-y^{i} \\right)x^{i}$, lets call the term from 1/m to end as gradient1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find gradient\n",
    "# capital X, capital Y denotes all values of X and Y\n",
    "def gradient(X,Y,theta):\n",
    "    m = X.shape[0] #number of examples.samples\n",
    "    \n",
    "    grad = np.zeros((2,))\n",
    "    # grad = array([0., 0.])\n",
    "\n",
    "    for i in range(m):\n",
    "        x = X[i]\n",
    "        y = Y[i]\n",
    "        #y prediction\n",
    "        y_ = hypothesis(x,theta)\n",
    "        grad[0] += (y_ - y)\n",
    "        grad[1] += (y_-y)*x\n",
    "    return grad/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(X,Y,theta):\n",
    "    m = X.shape[0]\n",
    "    total_error = 0.0\n",
    "    for i in range(m):\n",
    "        y_ = hypothesis(X[i],theta)\n",
    "        total_error += (y_ - Y[0])**2\n",
    "    return total_error/m\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,Y,max_step = 100, lr = 0.1):\n",
    "\n",
    "    #initialise theta\n",
    "    theta = np.zeros((2,))\n",
    "\n",
    "    error_list = []\n",
    "    \n",
    "    for i in range(max_step):\n",
    "        \n",
    "        grad = gradient(X,Y,theta)\n",
    "        e = error(X,Y,theta)\n",
    "        error_list.append(e)\n",
    "        #update theta\n",
    "        theta[0] -= lr*grad[0]\n",
    "        theta[1] -= lr*grad[1]\n",
    "    return theta,error_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "87afe836ff1f4c305c31657c167f0148cfb6c8693738850b0e6f3227727c76e1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
